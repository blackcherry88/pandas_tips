{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Essentials: Practical Best Practices\n",
    "\n",
    "A comprehensive guide to the most commonly used pandas operations for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# Main dataset\n",
    "df = pd.DataFrame({\n",
    "    'id': range(1, 101),\n",
    "    'name': [f'User_{i}' for i in range(1, 101)],\n",
    "    'age': np.random.randint(18, 65, 100),\n",
    "    'salary': np.random.randint(30000, 120000, 100),\n",
    "    'department': np.random.choice(['IT', 'HR', 'Finance', 'Marketing'], 100),\n",
    "    'join_date': pd.date_range('2020-01-01', periods=100, freq='3D'),\n",
    "    'score': np.random.uniform(0, 100, 100)\n",
    "})\n",
    "\n",
    "# Add some NaN values\n",
    "df.loc[np.random.choice(df.index, 10), 'salary'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 5), 'score'] = np.nan\n",
    "\n",
    "# Department info dataset for joining\n",
    "dept_info = pd.DataFrame({\n",
    "    'department': ['IT', 'HR', 'Finance', 'Marketing', 'Operations'],\n",
    "    'budget': [500000, 200000, 800000, 300000, 400000],\n",
    "    'location': ['Building A', 'Building B', 'Building C', 'Building A', 'Building B']\n",
    "})\n",
    "\n",
    "print(f\"Main dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Selecting Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select single column\n",
    "names = df['name']\n",
    "\n",
    "# Select multiple columns\n",
    "basic_info = df[['name', 'age', 'department']]\n",
    "\n",
    "# Select columns by position\n",
    "first_three_cols = df.iloc[:, :3]\n",
    "\n",
    "# Select rows by position\n",
    "first_five_rows = df.iloc[:5]\n",
    "\n",
    "# Select specific rows and columns\n",
    "subset = df.iloc[10:15, [0, 1, 3]]  # rows 10-14, columns 0,1,3\n",
    "\n",
    "# Select by label\n",
    "label_subset = df.loc[10:14, ['name', 'age', 'salary']]\n",
    "\n",
    "print(\"Basic info (first 3 rows):\")\n",
    "print(basic_info.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boolean Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single condition\n",
    "high_earners = df[df['salary'] > 80000]\n",
    "\n",
    "# Multiple conditions with &, |\n",
    "young_high_earners = df[(df['age'] < 30) & (df['salary'] > 70000)]\n",
    "\n",
    "# Using isin() for multiple values\n",
    "tech_finance = df[df['department'].isin(['IT', 'Finance'])]\n",
    "\n",
    "# String operations\n",
    "users_with_5 = df[df['name'].str.contains('5')]\n",
    "\n",
    "# Null value filtering\n",
    "complete_salary_data = df[df['salary'].notna()]\n",
    "\n",
    "# Query method (alternative syntax)\n",
    "query_result = df.query('age > 40 and department == \"IT\"')\n",
    "\n",
    "print(f\"High earners: {len(high_earners)} people\")\n",
    "print(f\"Young high earners: {len(young_high_earners)} people\")\n",
    "print(f\"Tech/Finance: {len(tech_finance)} people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Joining and Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join (default)\n",
    "df_with_dept = df.merge(dept_info, on='department')\n",
    "\n",
    "# Left join (keep all rows from left DataFrame)\n",
    "df_left = df.merge(dept_info, on='department', how='left')\n",
    "\n",
    "# Right join\n",
    "df_right = df.merge(dept_info, on='department', how='right')\n",
    "\n",
    "# Outer join (keep all rows from both)\n",
    "df_outer = df.merge(dept_info, on='department', how='outer')\n",
    "\n",
    "# Join on different column names\n",
    "# df.merge(other_df, left_on='col1', right_on='col2')\n",
    "\n",
    "# Join on index\n",
    "# df.merge(other_df, left_index=True, right_index=True)\n",
    "\n",
    "print(f\"Original df: {df.shape}\")\n",
    "print(f\"After inner join: {df_with_dept.shape}\")\n",
    "print(f\"After left join: {df_left.shape}\")\n",
    "print(f\"After outer join: {df_outer.shape}\")\n",
    "\n",
    "df_with_dept.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handling Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some duplicates for demonstration\n",
    "df_with_dupes = pd.concat([df, df.iloc[:5]], ignore_index=True)\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Total rows: {len(df_with_dupes)}\")\n",
    "print(f\"Duplicate rows: {df_with_dupes.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates (keep first occurrence)\n",
    "df_no_dupes = df_with_dupes.drop_duplicates()\n",
    "\n",
    "# Remove duplicates based on specific columns\n",
    "df_unique_names = df_with_dupes.drop_duplicates(subset=['name'])\n",
    "\n",
    "# Keep last occurrence instead of first\n",
    "df_keep_last = df_with_dupes.drop_duplicates(keep='last')\n",
    "\n",
    "# Find duplicate rows\n",
    "duplicate_rows = df_with_dupes[df_with_dupes.duplicated(keep=False)]\n",
    "\n",
    "print(f\"After removing duplicates: {len(df_no_dupes)}\")\n",
    "print(f\"Unique names only: {len(df_unique_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DateTime Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to datetime\n",
    "# df['date_col'] = pd.to_datetime(df['date_col'])\n",
    "\n",
    "# Extract date components\n",
    "df['join_year'] = df['join_date'].dt.year\n",
    "df['join_month'] = df['join_date'].dt.month\n",
    "df['join_weekday'] = df['join_date'].dt.day_name()\n",
    "\n",
    "# Calculate time differences\n",
    "df['days_since_join'] = (pd.Timestamp.now() - df['join_date']).dt.days\n",
    "\n",
    "# Filter by date range\n",
    "recent_joins = df[df['join_date'] >= '2020-06-01']\n",
    "\n",
    "# Resample by time period (requires datetime index)\n",
    "df_indexed = df.set_index('join_date')\n",
    "monthly_hires = df_indexed.resample('M').size()\n",
    "\n",
    "# Date arithmetic\n",
    "df['review_date'] = df['join_date'] + pd.DateOffset(months=6)\n",
    "\n",
    "print(\"Join date analysis:\")\n",
    "print(df[['name', 'join_date', 'join_year', 'join_weekday', 'days_since_join']].head())\n",
    "\n",
    "print(\"\\nMonthly hiring pattern:\")\n",
    "print(monthly_hires.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values with a constant\n",
    "df_filled_constant = df.fillna(0)\n",
    "\n",
    "# Fill with column mean/median\n",
    "df_filled_mean = df.copy()\n",
    "df_filled_mean['salary'] = df_filled_mean['salary'].fillna(df_filled_mean['salary'].mean())\n",
    "df_filled_mean['score'] = df_filled_mean['score'].fillna(df_filled_mean['score'].median())\n",
    "\n",
    "# Forward fill (use previous value)\n",
    "df_ffill = df.fillna(method='ffill')\n",
    "\n",
    "# Backward fill\n",
    "df_bfill = df.fillna(method='bfill')\n",
    "\n",
    "# Fill with different values per column\n",
    "fill_values = {'salary': df['salary'].median(), 'score': 0}\n",
    "df_custom_fill = df.fillna(value=fill_values)\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_no_na = df.dropna()\n",
    "\n",
    "# Drop rows with missing values in specific columns\n",
    "df_salary_complete = df.dropna(subset=['salary'])\n",
    "\n",
    "print(f\"\\nOriginal shape: {df.shape}\")\n",
    "print(f\"After dropping NAs: {df_no_na.shape}\")\n",
    "print(f\"After dropping salary NAs: {df_salary_complete.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample of rows\n",
    "sample_10 = df.sample(n=10, random_state=42)\n",
    "\n",
    "# Sample by percentage\n",
    "sample_20_percent = df.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# Sample with replacement\n",
    "sample_with_replacement = df.sample(n=50, replace=True, random_state=42)\n",
    "\n",
    "# Stratified sampling by group\n",
    "stratified_sample = df.groupby('department').apply(lambda x: x.sample(n=min(5, len(x)), random_state=42))\n",
    "stratified_sample = stratified_sample.reset_index(drop=True)\n",
    "\n",
    "# Sample top/bottom n rows\n",
    "top_earners = df.nlargest(10, 'salary')\n",
    "bottom_earners = df.nsmallest(10, 'salary')\n",
    "\n",
    "print(f\"Random sample: {sample_10.shape}\")\n",
    "print(f\"20% sample: {sample_20_percent.shape}\")\n",
    "print(f\"Stratified sample: {stratified_sample.shape}\")\n",
    "\n",
    "print(\"\\nTop 5 earners:\")\n",
    "print(top_earners[['name', 'salary', 'department']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Grouping and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic groupby operations\n",
    "dept_stats = df.groupby('department').agg({\n",
    "    'salary': ['mean', 'median', 'count'],\n",
    "    'age': 'mean',\n",
    "    'score': 'mean'\n",
    "})\n",
    "\n",
    "# Flatten column names\n",
    "dept_stats.columns = ['_'.join(col).strip() for col in dept_stats.columns]\n",
    "dept_stats = dept_stats.reset_index()\n",
    "\n",
    "# Multiple groupby columns\n",
    "age_dept_stats = df.groupby(['department', pd.cut(df['age'], bins=[0, 30, 50, 100], labels=['Young', 'Middle', 'Senior'])])['salary'].mean()\n",
    "\n",
    "# Apply custom function\n",
    "def salary_range(series):\n",
    "    return series.max() - series.min()\n",
    "\n",
    "salary_ranges = df.groupby('department')['salary'].apply(salary_range)\n",
    "\n",
    "print(\"Department statistics:\")\n",
    "print(dept_stats)\n",
    "\n",
    "print(\"\\nSalary ranges by department:\")\n",
    "print(salary_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_dept_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns\n",
    "df['salary_category'] = pd.cut(df['salary'], bins=[0, 50000, 80000, float('inf')], \n",
    "                              labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Apply function to column\n",
    "df['name_length'] = df['name'].apply(len)\n",
    "\n",
    "# Map values\n",
    "dept_mapping = {'IT': 'Technology', 'HR': 'Human Resources', 'Finance': 'Finance', 'Marketing': 'Marketing'}\n",
    "df['dept_full_name'] = df['department'].map(dept_mapping)\n",
    "\n",
    "# Conditional logic with np.where\n",
    "df['senior_employee'] = np.where(df['age'] >= 40, 'Senior', 'Junior')\n",
    "\n",
    "# Multiple conditions with np.select\n",
    "conditions = [\n",
    "    df['age'] < 30,\n",
    "    (df['age'] >= 30) & (df['age'] < 50),\n",
    "    df['age'] >= 50\n",
    "]\n",
    "choices = ['Young', 'Middle-aged', 'Senior']\n",
    "df['age_group'] = np.select(conditions, choices, default='Unknown')\n",
    "\n",
    "# Rank data\n",
    "df['salary_rank'] = df['salary'].rank(ascending=False)\n",
    "\n",
    "print(\"Transformed data sample:\")\n",
    "print(df[['name', 'age', 'salary', 'salary_category', 'age_group', 'salary_rank']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Pivot Tables and Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table\n",
    "pivot_table = df.pivot_table(\n",
    "    values='salary', \n",
    "    index='department', \n",
    "    columns='age_group', \n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Cross-tabulation\n",
    "crosstab = pd.crosstab(df['department'], df['age_group'], margins=True)\n",
    "\n",
    "# Melt (wide to long format)\n",
    "df_subset = df[['name', 'age', 'salary', 'score']].head(5)\n",
    "melted = df_subset.melt(id_vars=['name'], value_vars=['age', 'salary', 'score'])\n",
    "\n",
    "print(\"Pivot table - Average salary by department and age group:\")\n",
    "print(pivot_table)\n",
    "\n",
    "print(\"\\nCross-tabulation:\")\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. String Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String methods\n",
    "df['name_upper'] = df['name'].str.upper()\n",
    "df['name_lower'] = df['name'].str.lower()\n",
    "\n",
    "# Extract parts of strings\n",
    "df['user_number'] = df['name'].str.extract(r'User_(\\d+)').astype(int)\n",
    "\n",
    "# String contains\n",
    "contains_1 = df[df['name'].str.contains('1')]\n",
    "\n",
    "# String replacement\n",
    "df['name_modified'] = df['name'].str.replace('User_', 'Employee_')\n",
    "\n",
    "# Split strings\n",
    "df['name_parts'] = df['name'].str.split('_')\n",
    "df['prefix'] = df['name_parts'].str[0]\n",
    "df['number'] = df['name_parts'].str[1]\n",
    "\n",
    "print(\"String operations sample:\")\n",
    "print(df[['name', 'name_upper', 'user_number', 'name_modified']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vectorized operations instead of loops\n",
    "# Good: df['new_col'] = df['col1'] * df['col2']\n",
    "# Bad: df['new_col'] = df.apply(lambda x: x['col1'] * x['col2'], axis=1)\n",
    "\n",
    "# Use categorical data for repeated strings\n",
    "df['department_cat'] = df['department'].astype('category')\n",
    "print(f\"Memory usage - original: {df['department'].memory_usage(deep=True)} bytes\")\n",
    "print(f\"Memory usage - categorical: {df['department_cat'].memory_usage(deep=True)} bytes\")\n",
    "\n",
    "# Use query() for complex filtering (can be faster)\n",
    "# df.query('age > 30 and salary > 50000')\n",
    "\n",
    "# Use loc/iloc for explicit indexing\n",
    "# df.loc[df['age'] > 30, 'salary'] *= 1.1\n",
    "\n",
    "# Chain operations efficiently\n",
    "result = (df\n",
    "          .query('age > 25')\n",
    "          .groupby('department')['salary']\n",
    "          .mean()\n",
    "          .sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nChained operation result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Quick Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Value counts\n",
    "print(\"\\nDepartment distribution:\")\n",
    "print(df['department'].value_counts())\n",
    "\n",
    "# Correlation matrix\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covers the most essential pandas operations:\n",
    "\n",
    "1. **Data Selection**: iloc, loc, boolean indexing\n",
    "2. **Joining**: merge operations with different join types\n",
    "3. **Duplicates**: detection and removal\n",
    "4. **DateTime**: parsing, extraction, arithmetic\n",
    "5. **Missing Values**: detection, filling, dropping\n",
    "6. **Sampling**: random, stratified, top/bottom\n",
    "7. **Grouping**: aggregation and transformation\n",
    "8. **Data Transformation**: new columns, mapping, ranking\n",
    "9. **Reshaping**: pivot tables, melting\n",
    "10. **String Operations**: cleaning and extraction\n",
    "11. **Performance**: memory optimization and efficient operations\n",
    "12. **Exploration**: quick data overview methods\n",
    "\n",
    "These operations cover 90% of typical data manipulation tasks in pandas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
